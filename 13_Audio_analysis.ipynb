{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13- Audio analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# **Analysis of the audio results with the window slide technique**"
      ],
      "metadata": {
        "id": "6FF8M28CyAyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHm-oosax86W",
        "outputId": "9d87819c-cd35-47dc-8f36-a91dd6bc3a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-19 18:03:02--  https://perso.esiee.fr/~gueurett/LV_Research/WS_analysis_results.csv\n",
            "Resolving perso.esiee.fr (perso.esiee.fr)... 147.215.150.8\n",
            "Connecting to perso.esiee.fr (perso.esiee.fr)|147.215.150.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 958567 (936K) [text/csv]\n",
            "Saving to: ‘WS_analysis_results.csv’\n",
            "\n",
            "WS_analysis_results 100%[===================>] 936.10K  1.65MB/s    in 0.6s    \n",
            "\n",
            "2022-07-19 18:03:03 (1.65 MB/s) - ‘WS_analysis_results.csv’ saved [958567/958567]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://perso.esiee.fr/~gueurett/LV_Research/WS_analysis_results.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://perso.esiee.fr/~gueurett/LV_Research/Acoustic_Analysis_Results.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw9Joau5Ri5W",
        "outputId": "6667a693-418a-483c-a928-3bf51e51795a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-19 18:03:05--  https://perso.esiee.fr/~gueurett/LV_Research/Acoustic_Analysis_Results.csv\n",
            "Resolving perso.esiee.fr (perso.esiee.fr)... 147.215.150.8\n",
            "Connecting to perso.esiee.fr (perso.esiee.fr)|147.215.150.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68157 (67K) [text/csv]\n",
            "Saving to: ‘Acoustic_Analysis_Results.csv’\n",
            "\n",
            "Acoustic_Analysis_R 100%[===================>]  66.56K   362KB/s    in 0.2s    \n",
            "\n",
            "2022-07-19 18:03:06 (362 KB/s) - ‘Acoustic_Analysis_Results.csv’ saved [68157/68157]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from keras import layers, initializers\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "nfToGUy00t8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prints = False\n",
        "\n",
        "data = pd.read_csv('WS_analysis_results.csv', delimiter=',', dtype=None, encoding=None)\n",
        "data = data.rename(columns={'MOMENTS': 'Filename'}) # rename the MOMENTS column to the filename column\n",
        "\n",
        "data_2 = pd.read_csv('Acoustic_Analysis_Results.csv', delimiter=',', dtype=None, encoding=None)\n",
        "data_2 = data_2.rename(columns={'Unnamed: 0': 'Filename'}) # rename the not-named column to the filename column\n",
        "data_2 = data_2.drop(columns=['Filename'])\n",
        "data_2 = data_2.fillna(data_2.mean())\n",
        "\n",
        "for column_name in data_2.columns:\n",
        "  data_2[column_name] = (data_2[column_name] - np.mean(data_2[column_name])) / np.std(data_2[column_name])\n",
        "data_2 = data_2.to_numpy()\n",
        "\n",
        "n_samples = 200\n",
        "n_windows = 14\n",
        "n_features = 35\n",
        "\n",
        "dataset = np.ones(shape=(n_samples, n_windows+1, n_features))\n",
        "labels  = np.zeros(shape=(n_samples))\n",
        "\n",
        "for column_name in data.columns:\n",
        "  if column_name == 'Filename': continue\n",
        "  data[column_name] = (data[column_name] - np.mean(data[column_name])) / np.std(data[column_name])\n",
        "\n",
        "for index, row in data.iterrows():\n",
        "\n",
        "  filename = row['Filename'][:-4]\n",
        "  if prints: print(\"filename without wav: \", filename)\n",
        "\n",
        "  label, n_sample, n_part = filename.split('_')\n",
        "  n_sample = int(n_sample)\n",
        "  n_part   = int(n_part)\n",
        "\n",
        "  if prints:\n",
        "    print(\"label: \", label)\n",
        "    print(\"n_sample: \", n_sample)\n",
        "    print(\"n_part: \", n_part)\n",
        "\n",
        "  if n_part == 9 and label == 'I':    # there are 14 parts but the index 9 is the last one because in the csv file, the filenames are sorted such as 9 is at the end\n",
        "      labels[n_sample] = 1\n",
        "\n",
        "  row = data.iloc[index].to_numpy()[1:]\n",
        "  if prints:\n",
        "    print(\"row.shape: \", row.shape)\n",
        "    print(\"dataset.shape:  \", dataset.shape)\n",
        "    print(\"dataset[n_sample, n_part, :].shape:  \", dataset[n_sample, n_part, :].shape, '\\n\\n')\n",
        "  dataset[n_sample, n_part, :] = row\n",
        "\n",
        "for index in range(n_samples):\n",
        "  dataset[index, -1, :] = data_2[index]\n",
        "\n",
        "rng     = np.random.default_rng() # shuffle time\n",
        "shuffle = rng.choice(n_samples, size=(n_samples), replace=False)\n",
        "data    = dataset[shuffle]\n",
        "labels  = labels[shuffle]\n",
        "\n",
        "print(\"---------------------------------\")\n",
        "print(\"|  dataset.shape:\", dataset.shape, \"|\")\n",
        "print(\"|  labels.shape: \", labels.shape, \"       |\")\n",
        "print(\"---------------------------------\")"
      ],
      "metadata": {
        "id": "FqkHULh40vQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41adac1f-c784-44b3-8b9f-7998d0061c59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "|  dataset.shape: (200, 15, 35) |\n",
            "|  labels.shape:  (200,)        |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_things(history):\n",
        "  fig = plt.figure(figsize=(10, 8))\n",
        "  plt.subplot(2,1,1)\n",
        "  plt.plot(history.history['binary_accuracy'])\n",
        "  plt.plot(history.history['val_binary_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 8))\n",
        "  plt.subplot(2,1,2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "5CDmsKTe9dqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_1(input_shape=(15, 35), n_neurons=50, dropout_rate=0.2, learning_rate=0.0001, beta_1=0.2):\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Flatten(input_shape=input_shape))\n",
        "\n",
        "  model.add(layers.Dense(n_neurons))\n",
        "  model.add(layers.ReLU())\n",
        "  model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  optimizer = Adam(learning_rate, beta_1=beta_1)\n",
        "  model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "model = get_model_1()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOy-fwzf7Ba6",
        "outputId": "7d7f9b00-cc9e-414b-d654-7cdf120a867e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_114\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_114 (Flatten)       (None, 525)               0         \n",
            "                                                                 \n",
            " dense_290 (Dense)           (None, 50)                26300     \n",
            "                                                                 \n",
            " re_lu_176 (ReLU)            (None, 50)                0         \n",
            "                                                                 \n",
            " dropout_176 (Dropout)       (None, 50)                0         \n",
            "                                                                 \n",
            " dense_291 (Dense)           (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,351\n",
            "Trainable params: 26,351\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, labels, epochs=200, batch_size=10, validation_split=0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJPWaa3k79Ok",
        "outputId": "090f4585-db01-40d2-cb92-2293a069a067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 1s 18ms/step - loss: 0.7730 - binary_accuracy: 0.5333 - val_loss: 0.5188 - val_binary_accuracy: 0.7500\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7254 - binary_accuracy: 0.5500 - val_loss: 0.5152 - val_binary_accuracy: 0.7500\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6744 - binary_accuracy: 0.5750 - val_loss: 0.5138 - val_binary_accuracy: 0.7500\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7203 - binary_accuracy: 0.6000 - val_loss: 0.5107 - val_binary_accuracy: 0.7750\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6613 - binary_accuracy: 0.6583 - val_loss: 0.5069 - val_binary_accuracy: 0.7750\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6418 - binary_accuracy: 0.6333 - val_loss: 0.5057 - val_binary_accuracy: 0.7875\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6400 - binary_accuracy: 0.6417 - val_loss: 0.5046 - val_binary_accuracy: 0.7875\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5813 - binary_accuracy: 0.6500 - val_loss: 0.5034 - val_binary_accuracy: 0.7875\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6045 - binary_accuracy: 0.6583 - val_loss: 0.5024 - val_binary_accuracy: 0.7875\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5993 - binary_accuracy: 0.6667 - val_loss: 0.5009 - val_binary_accuracy: 0.7875\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5309 - binary_accuracy: 0.7500 - val_loss: 0.5016 - val_binary_accuracy: 0.7875\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5627 - binary_accuracy: 0.7333 - val_loss: 0.4996 - val_binary_accuracy: 0.7875\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5173 - binary_accuracy: 0.7250 - val_loss: 0.4987 - val_binary_accuracy: 0.7875\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4628 - binary_accuracy: 0.7917 - val_loss: 0.4978 - val_binary_accuracy: 0.7875\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5245 - binary_accuracy: 0.7250 - val_loss: 0.4994 - val_binary_accuracy: 0.7875\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4540 - binary_accuracy: 0.7917 - val_loss: 0.5014 - val_binary_accuracy: 0.7875\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4506 - binary_accuracy: 0.7750 - val_loss: 0.5006 - val_binary_accuracy: 0.7875\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4417 - binary_accuracy: 0.8000 - val_loss: 0.5012 - val_binary_accuracy: 0.7875\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4323 - binary_accuracy: 0.8083 - val_loss: 0.4993 - val_binary_accuracy: 0.7875\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4239 - binary_accuracy: 0.8167 - val_loss: 0.4983 - val_binary_accuracy: 0.7875\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4071 - binary_accuracy: 0.8333 - val_loss: 0.4979 - val_binary_accuracy: 0.7875\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4320 - binary_accuracy: 0.7917 - val_loss: 0.4981 - val_binary_accuracy: 0.7875\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4054 - binary_accuracy: 0.8250 - val_loss: 0.4992 - val_binary_accuracy: 0.8000\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4065 - binary_accuracy: 0.8167 - val_loss: 0.4992 - val_binary_accuracy: 0.8000\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3798 - binary_accuracy: 0.8667 - val_loss: 0.5030 - val_binary_accuracy: 0.8000\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3896 - binary_accuracy: 0.8583 - val_loss: 0.5075 - val_binary_accuracy: 0.8000\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3765 - binary_accuracy: 0.8667 - val_loss: 0.5097 - val_binary_accuracy: 0.8000\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3509 - binary_accuracy: 0.8833 - val_loss: 0.5107 - val_binary_accuracy: 0.8000\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3294 - binary_accuracy: 0.9083 - val_loss: 0.5118 - val_binary_accuracy: 0.8000\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3481 - binary_accuracy: 0.8833 - val_loss: 0.5124 - val_binary_accuracy: 0.8000\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3119 - binary_accuracy: 0.9000 - val_loss: 0.5094 - val_binary_accuracy: 0.8000\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3098 - binary_accuracy: 0.8917 - val_loss: 0.5116 - val_binary_accuracy: 0.8000\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3331 - binary_accuracy: 0.8917 - val_loss: 0.5146 - val_binary_accuracy: 0.8000\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2920 - binary_accuracy: 0.9250 - val_loss: 0.5127 - val_binary_accuracy: 0.8000\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3064 - binary_accuracy: 0.9167 - val_loss: 0.5143 - val_binary_accuracy: 0.8000\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2947 - binary_accuracy: 0.9417 - val_loss: 0.5174 - val_binary_accuracy: 0.8000\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2847 - binary_accuracy: 0.9250 - val_loss: 0.5173 - val_binary_accuracy: 0.8000\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2973 - binary_accuracy: 0.9000 - val_loss: 0.5181 - val_binary_accuracy: 0.8000\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2915 - binary_accuracy: 0.9083 - val_loss: 0.5183 - val_binary_accuracy: 0.8000\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3011 - binary_accuracy: 0.8917 - val_loss: 0.5172 - val_binary_accuracy: 0.8000\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2834 - binary_accuracy: 0.9167 - val_loss: 0.5176 - val_binary_accuracy: 0.8000\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2594 - binary_accuracy: 0.9333 - val_loss: 0.5194 - val_binary_accuracy: 0.8000\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2542 - binary_accuracy: 0.9667 - val_loss: 0.5203 - val_binary_accuracy: 0.8000\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2458 - binary_accuracy: 0.9417 - val_loss: 0.5208 - val_binary_accuracy: 0.8000\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2710 - binary_accuracy: 0.9333 - val_loss: 0.5205 - val_binary_accuracy: 0.8000\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2379 - binary_accuracy: 0.9667 - val_loss: 0.5243 - val_binary_accuracy: 0.8000\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2485 - binary_accuracy: 0.9667 - val_loss: 0.5272 - val_binary_accuracy: 0.8000\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2448 - binary_accuracy: 0.9333 - val_loss: 0.5237 - val_binary_accuracy: 0.8000\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2561 - binary_accuracy: 0.9417 - val_loss: 0.5251 - val_binary_accuracy: 0.8000\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2181 - binary_accuracy: 0.9583 - val_loss: 0.5277 - val_binary_accuracy: 0.8000\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2473 - binary_accuracy: 0.9167 - val_loss: 0.5279 - val_binary_accuracy: 0.8000\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2150 - binary_accuracy: 0.9667 - val_loss: 0.5313 - val_binary_accuracy: 0.8000\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2243 - binary_accuracy: 0.9500 - val_loss: 0.5345 - val_binary_accuracy: 0.8000\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.2180 - binary_accuracy: 0.9667 - val_loss: 0.5358 - val_binary_accuracy: 0.8000\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2133 - binary_accuracy: 0.9833 - val_loss: 0.5376 - val_binary_accuracy: 0.8000\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1915 - binary_accuracy: 0.9750 - val_loss: 0.5392 - val_binary_accuracy: 0.8000\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1856 - binary_accuracy: 0.9667 - val_loss: 0.5378 - val_binary_accuracy: 0.8000\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1789 - binary_accuracy: 0.9917 - val_loss: 0.5415 - val_binary_accuracy: 0.8000\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1705 - binary_accuracy: 0.9833 - val_loss: 0.5409 - val_binary_accuracy: 0.8000\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1744 - binary_accuracy: 0.9833 - val_loss: 0.5448 - val_binary_accuracy: 0.8000\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1691 - binary_accuracy: 0.9833 - val_loss: 0.5470 - val_binary_accuracy: 0.8000\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1739 - binary_accuracy: 0.9917 - val_loss: 0.5477 - val_binary_accuracy: 0.8000\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1629 - binary_accuracy: 0.9750 - val_loss: 0.5525 - val_binary_accuracy: 0.8000\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1719 - binary_accuracy: 0.9667 - val_loss: 0.5543 - val_binary_accuracy: 0.8000\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1816 - binary_accuracy: 0.9667 - val_loss: 0.5513 - val_binary_accuracy: 0.8000\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1649 - binary_accuracy: 0.9917 - val_loss: 0.5532 - val_binary_accuracy: 0.8000\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1769 - binary_accuracy: 0.9667 - val_loss: 0.5561 - val_binary_accuracy: 0.8000\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1543 - binary_accuracy: 0.9917 - val_loss: 0.5576 - val_binary_accuracy: 0.8000\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1548 - binary_accuracy: 0.9917 - val_loss: 0.5560 - val_binary_accuracy: 0.8000\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1573 - binary_accuracy: 1.0000 - val_loss: 0.5588 - val_binary_accuracy: 0.8000\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1425 - binary_accuracy: 0.9917 - val_loss: 0.5589 - val_binary_accuracy: 0.8000\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1389 - binary_accuracy: 1.0000 - val_loss: 0.5629 - val_binary_accuracy: 0.8000\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1497 - binary_accuracy: 0.9917 - val_loss: 0.5682 - val_binary_accuracy: 0.8000\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1492 - binary_accuracy: 0.9833 - val_loss: 0.5683 - val_binary_accuracy: 0.8000\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1306 - binary_accuracy: 0.9917 - val_loss: 0.5702 - val_binary_accuracy: 0.8000\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1368 - binary_accuracy: 0.9917 - val_loss: 0.5720 - val_binary_accuracy: 0.8000\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1473 - binary_accuracy: 0.9750 - val_loss: 0.5745 - val_binary_accuracy: 0.8000\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1555 - binary_accuracy: 0.9917 - val_loss: 0.5757 - val_binary_accuracy: 0.8000\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1247 - binary_accuracy: 0.9917 - val_loss: 0.5774 - val_binary_accuracy: 0.8000\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1251 - binary_accuracy: 0.9917 - val_loss: 0.5782 - val_binary_accuracy: 0.8000\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1248 - binary_accuracy: 0.9917 - val_loss: 0.5773 - val_binary_accuracy: 0.8000\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1243 - binary_accuracy: 0.9917 - val_loss: 0.5773 - val_binary_accuracy: 0.8000\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1217 - binary_accuracy: 1.0000 - val_loss: 0.5811 - val_binary_accuracy: 0.8000\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1146 - binary_accuracy: 1.0000 - val_loss: 0.5826 - val_binary_accuracy: 0.8000\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1131 - binary_accuracy: 1.0000 - val_loss: 0.5876 - val_binary_accuracy: 0.8000\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1216 - binary_accuracy: 0.9917 - val_loss: 0.5849 - val_binary_accuracy: 0.8000\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1232 - binary_accuracy: 1.0000 - val_loss: 0.5912 - val_binary_accuracy: 0.8000\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1063 - binary_accuracy: 1.0000 - val_loss: 0.5911 - val_binary_accuracy: 0.8000\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1221 - binary_accuracy: 0.9917 - val_loss: 0.5894 - val_binary_accuracy: 0.8000\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1140 - binary_accuracy: 0.9917 - val_loss: 0.5931 - val_binary_accuracy: 0.8000\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1011 - binary_accuracy: 1.0000 - val_loss: 0.5955 - val_binary_accuracy: 0.8000\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1097 - binary_accuracy: 1.0000 - val_loss: 0.5939 - val_binary_accuracy: 0.8000\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0949 - binary_accuracy: 1.0000 - val_loss: 0.5967 - val_binary_accuracy: 0.8000\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1028 - binary_accuracy: 1.0000 - val_loss: 0.5996 - val_binary_accuracy: 0.8000\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0981 - binary_accuracy: 1.0000 - val_loss: 0.6002 - val_binary_accuracy: 0.8000\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1075 - binary_accuracy: 1.0000 - val_loss: 0.6007 - val_binary_accuracy: 0.8000\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0981 - binary_accuracy: 0.9917 - val_loss: 0.6019 - val_binary_accuracy: 0.8000\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1000 - binary_accuracy: 0.9917 - val_loss: 0.6027 - val_binary_accuracy: 0.8000\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0887 - binary_accuracy: 1.0000 - val_loss: 0.6045 - val_binary_accuracy: 0.8000\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0955 - binary_accuracy: 1.0000 - val_loss: 0.6021 - val_binary_accuracy: 0.8000\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0905 - binary_accuracy: 1.0000 - val_loss: 0.6082 - val_binary_accuracy: 0.8000\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0962 - binary_accuracy: 1.0000 - val_loss: 0.6103 - val_binary_accuracy: 0.8000\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0882 - binary_accuracy: 1.0000 - val_loss: 0.6093 - val_binary_accuracy: 0.8000\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0889 - binary_accuracy: 1.0000 - val_loss: 0.6094 - val_binary_accuracy: 0.8000\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0778 - binary_accuracy: 1.0000 - val_loss: 0.6140 - val_binary_accuracy: 0.8000\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0843 - binary_accuracy: 1.0000 - val_loss: 0.6177 - val_binary_accuracy: 0.8000\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0884 - binary_accuracy: 0.9917 - val_loss: 0.6200 - val_binary_accuracy: 0.8000\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0862 - binary_accuracy: 1.0000 - val_loss: 0.6222 - val_binary_accuracy: 0.8000\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0781 - binary_accuracy: 1.0000 - val_loss: 0.6207 - val_binary_accuracy: 0.8000\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0977 - binary_accuracy: 1.0000 - val_loss: 0.6196 - val_binary_accuracy: 0.8000\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0798 - binary_accuracy: 1.0000 - val_loss: 0.6192 - val_binary_accuracy: 0.8000\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0896 - binary_accuracy: 0.9917 - val_loss: 0.6193 - val_binary_accuracy: 0.8000\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0745 - binary_accuracy: 1.0000 - val_loss: 0.6225 - val_binary_accuracy: 0.8000\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0736 - binary_accuracy: 1.0000 - val_loss: 0.6248 - val_binary_accuracy: 0.8000\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0683 - binary_accuracy: 1.0000 - val_loss: 0.6256 - val_binary_accuracy: 0.8000\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0730 - binary_accuracy: 1.0000 - val_loss: 0.6233 - val_binary_accuracy: 0.8000\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0722 - binary_accuracy: 1.0000 - val_loss: 0.6298 - val_binary_accuracy: 0.8000\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0776 - binary_accuracy: 1.0000 - val_loss: 0.6275 - val_binary_accuracy: 0.8000\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0753 - binary_accuracy: 1.0000 - val_loss: 0.6326 - val_binary_accuracy: 0.8000\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0714 - binary_accuracy: 1.0000 - val_loss: 0.6345 - val_binary_accuracy: 0.8000\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0574 - binary_accuracy: 1.0000 - val_loss: 0.6388 - val_binary_accuracy: 0.8000\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0618 - binary_accuracy: 1.0000 - val_loss: 0.6422 - val_binary_accuracy: 0.8000\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0690 - binary_accuracy: 1.0000 - val_loss: 0.6412 - val_binary_accuracy: 0.8000\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0652 - binary_accuracy: 0.9917 - val_loss: 0.6407 - val_binary_accuracy: 0.8000\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0622 - binary_accuracy: 1.0000 - val_loss: 0.6407 - val_binary_accuracy: 0.8000\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0633 - binary_accuracy: 1.0000 - val_loss: 0.6421 - val_binary_accuracy: 0.8000\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0617 - binary_accuracy: 1.0000 - val_loss: 0.6476 - val_binary_accuracy: 0.8000\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0604 - binary_accuracy: 1.0000 - val_loss: 0.6477 - val_binary_accuracy: 0.8000\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0644 - binary_accuracy: 0.9917 - val_loss: 0.6528 - val_binary_accuracy: 0.8000\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0619 - binary_accuracy: 1.0000 - val_loss: 0.6592 - val_binary_accuracy: 0.8000\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0709 - binary_accuracy: 1.0000 - val_loss: 0.6562 - val_binary_accuracy: 0.8000\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0624 - binary_accuracy: 1.0000 - val_loss: 0.6592 - val_binary_accuracy: 0.8000\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0616 - binary_accuracy: 1.0000 - val_loss: 0.6545 - val_binary_accuracy: 0.8000\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0530 - binary_accuracy: 1.0000 - val_loss: 0.6569 - val_binary_accuracy: 0.8000\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0596 - binary_accuracy: 1.0000 - val_loss: 0.6600 - val_binary_accuracy: 0.8000\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0539 - binary_accuracy: 1.0000 - val_loss: 0.6650 - val_binary_accuracy: 0.8000\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0481 - binary_accuracy: 1.0000 - val_loss: 0.6711 - val_binary_accuracy: 0.8000\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0536 - binary_accuracy: 1.0000 - val_loss: 0.6704 - val_binary_accuracy: 0.8000\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0534 - binary_accuracy: 1.0000 - val_loss: 0.6685 - val_binary_accuracy: 0.8000\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0482 - binary_accuracy: 1.0000 - val_loss: 0.6719 - val_binary_accuracy: 0.8000\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0501 - binary_accuracy: 1.0000 - val_loss: 0.6699 - val_binary_accuracy: 0.8000\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0549 - binary_accuracy: 1.0000 - val_loss: 0.6710 - val_binary_accuracy: 0.8000\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0473 - binary_accuracy: 1.0000 - val_loss: 0.6736 - val_binary_accuracy: 0.8000\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0535 - binary_accuracy: 1.0000 - val_loss: 0.6788 - val_binary_accuracy: 0.8000\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0476 - binary_accuracy: 1.0000 - val_loss: 0.6798 - val_binary_accuracy: 0.8000\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0503 - binary_accuracy: 1.0000 - val_loss: 0.6780 - val_binary_accuracy: 0.8000\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0411 - binary_accuracy: 1.0000 - val_loss: 0.6791 - val_binary_accuracy: 0.8000\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0448 - binary_accuracy: 1.0000 - val_loss: 0.6828 - val_binary_accuracy: 0.8000\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0456 - binary_accuracy: 1.0000 - val_loss: 0.6808 - val_binary_accuracy: 0.8000\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0428 - binary_accuracy: 1.0000 - val_loss: 0.6877 - val_binary_accuracy: 0.8000\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0485 - binary_accuracy: 1.0000 - val_loss: 0.6837 - val_binary_accuracy: 0.8000\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0427 - binary_accuracy: 1.0000 - val_loss: 0.6870 - val_binary_accuracy: 0.8000\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0371 - binary_accuracy: 1.0000 - val_loss: 0.6916 - val_binary_accuracy: 0.8000\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0371 - binary_accuracy: 1.0000 - val_loss: 0.6935 - val_binary_accuracy: 0.8000\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0411 - binary_accuracy: 1.0000 - val_loss: 0.6966 - val_binary_accuracy: 0.8000\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0447 - binary_accuracy: 1.0000 - val_loss: 0.6979 - val_binary_accuracy: 0.8000\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0486 - binary_accuracy: 1.0000 - val_loss: 0.6965 - val_binary_accuracy: 0.8000\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0450 - binary_accuracy: 1.0000 - val_loss: 0.6947 - val_binary_accuracy: 0.8000\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0392 - binary_accuracy: 1.0000 - val_loss: 0.7005 - val_binary_accuracy: 0.8000\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0428 - binary_accuracy: 1.0000 - val_loss: 0.6980 - val_binary_accuracy: 0.8000\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0471 - binary_accuracy: 1.0000 - val_loss: 0.7037 - val_binary_accuracy: 0.8000\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0424 - binary_accuracy: 1.0000 - val_loss: 0.7053 - val_binary_accuracy: 0.8000\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0322 - binary_accuracy: 1.0000 - val_loss: 0.7076 - val_binary_accuracy: 0.8000\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0418 - binary_accuracy: 1.0000 - val_loss: 0.7059 - val_binary_accuracy: 0.8000\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0399 - binary_accuracy: 1.0000 - val_loss: 0.7102 - val_binary_accuracy: 0.8000\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0327 - binary_accuracy: 1.0000 - val_loss: 0.7099 - val_binary_accuracy: 0.8000\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0320 - binary_accuracy: 1.0000 - val_loss: 0.7150 - val_binary_accuracy: 0.8000\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0384 - binary_accuracy: 1.0000 - val_loss: 0.7168 - val_binary_accuracy: 0.8000\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0327 - binary_accuracy: 1.0000 - val_loss: 0.7188 - val_binary_accuracy: 0.8000\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0320 - binary_accuracy: 1.0000 - val_loss: 0.7225 - val_binary_accuracy: 0.8000\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0406 - binary_accuracy: 1.0000 - val_loss: 0.7234 - val_binary_accuracy: 0.8000\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0348 - binary_accuracy: 1.0000 - val_loss: 0.7262 - val_binary_accuracy: 0.8000\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0365 - binary_accuracy: 1.0000 - val_loss: 0.7191 - val_binary_accuracy: 0.8000\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0312 - binary_accuracy: 1.0000 - val_loss: 0.7206 - val_binary_accuracy: 0.8000\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0318 - binary_accuracy: 1.0000 - val_loss: 0.7190 - val_binary_accuracy: 0.7875\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0317 - binary_accuracy: 1.0000 - val_loss: 0.7196 - val_binary_accuracy: 0.7875\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0312 - binary_accuracy: 1.0000 - val_loss: 0.7246 - val_binary_accuracy: 0.7875\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0318 - binary_accuracy: 1.0000 - val_loss: 0.7248 - val_binary_accuracy: 0.7875\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0277 - binary_accuracy: 1.0000 - val_loss: 0.7239 - val_binary_accuracy: 0.7875\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0343 - binary_accuracy: 1.0000 - val_loss: 0.7265 - val_binary_accuracy: 0.7875\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0293 - binary_accuracy: 1.0000 - val_loss: 0.7265 - val_binary_accuracy: 0.7875\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0339 - binary_accuracy: 1.0000 - val_loss: 0.7284 - val_binary_accuracy: 0.7875\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0279 - binary_accuracy: 1.0000 - val_loss: 0.7347 - val_binary_accuracy: 0.7875\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0282 - binary_accuracy: 1.0000 - val_loss: 0.7380 - val_binary_accuracy: 0.7875\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0266 - binary_accuracy: 1.0000 - val_loss: 0.7410 - val_binary_accuracy: 0.7875\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0290 - binary_accuracy: 1.0000 - val_loss: 0.7417 - val_binary_accuracy: 0.7875\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0298 - binary_accuracy: 1.0000 - val_loss: 0.7405 - val_binary_accuracy: 0.7875\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0250 - binary_accuracy: 1.0000 - val_loss: 0.7410 - val_binary_accuracy: 0.7875\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0326 - binary_accuracy: 1.0000 - val_loss: 0.7400 - val_binary_accuracy: 0.7875\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0326 - binary_accuracy: 1.0000 - val_loss: 0.7445 - val_binary_accuracy: 0.8000\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0273 - binary_accuracy: 1.0000 - val_loss: 0.7468 - val_binary_accuracy: 0.8000\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0251 - binary_accuracy: 1.0000 - val_loss: 0.7418 - val_binary_accuracy: 0.7875\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0263 - binary_accuracy: 1.0000 - val_loss: 0.7446 - val_binary_accuracy: 0.7875\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0276 - binary_accuracy: 1.0000 - val_loss: 0.7446 - val_binary_accuracy: 0.7875\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0246 - binary_accuracy: 1.0000 - val_loss: 0.7429 - val_binary_accuracy: 0.7875\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0239 - binary_accuracy: 1.0000 - val_loss: 0.7495 - val_binary_accuracy: 0.7875\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0258 - binary_accuracy: 1.0000 - val_loss: 0.7497 - val_binary_accuracy: 0.7875\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0254 - binary_accuracy: 1.0000 - val_loss: 0.7497 - val_binary_accuracy: 0.7875\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0189 - binary_accuracy: 1.0000 - val_loss: 0.7510 - val_binary_accuracy: 0.7875\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0255 - binary_accuracy: 1.0000 - val_loss: 0.7567 - val_binary_accuracy: 0.7875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_things(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "TakkartuAowU",
        "outputId": "645ca873-b5d8-4869-975d-a40c8e3ae949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_splits = 8\n",
        "number_runs   = 3\n",
        "results_acc   = np.zeros(shape=(number_runs, number_splits))\n",
        "results_loss  = np.zeros(shape=(number_runs, number_splits))\n",
        "\n",
        "\n",
        "for run in range(number_runs):\n",
        "  \n",
        "  print(\"\\n\\n\\nDoing the run number \", run+1)\n",
        "  kfold = KFold(n_splits=number_splits, shuffle=True)\n",
        "\n",
        "  # K-fold Cross Validation model evaluation\n",
        "  fold_no = 1\n",
        "\n",
        "  for split,(fold_train, fold_test) in enumerate(kfold.split(dataset, labels)):\n",
        "\n",
        "    model = get_model_1()\n",
        "    print(\"Training the fold number \", fold_no,\"\\n\")\n",
        "    history = model.fit(dataset[fold_train], labels[fold_train], batch_size=10, epochs=25)\n",
        "\n",
        "    scores = model.evaluate(dataset[fold_test], labels[fold_test], verbose=0)\n",
        "    print(\"For the fold number \",  fold_no, \":\\nloss = \", scores[0], \"\\naccuracy = \", scores[1]*100,\"%\")\n",
        "    results_acc[run][split] = scores[1] * 100\n",
        "    results_loss[run][split] = scores[0]\n",
        "\n",
        "    # Increase fold number\n",
        "    fold_no = fold_no + 1\n",
        "\n",
        "\n",
        "results_acc = np.mean(results_acc, axis=1)\n",
        "results_loss = np.mean(results_loss, axis=1)\n",
        "\n",
        "results_acc = np.mean(results_acc)\n",
        "results_loss = np.mean(results_loss)\n",
        "\n",
        "print(\"\\n\\n\\nWe obtain the following results:\\nmean accuracy: \",results_acc,\"%\", \"\\nmean loss:\",results_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0uKso7UMY0I",
        "outputId": "c5f00af5-e7a6-4ac1-bc41-82a211f9f519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Doing the run number  1\n",
            "Training the fold number  1 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 4ms/step - loss: 0.6598 - binary_accuracy: 0.6400\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6566 - binary_accuracy: 0.6400\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6183 - binary_accuracy: 0.6571\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6425 - binary_accuracy: 0.6800\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5965 - binary_accuracy: 0.6971\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5994 - binary_accuracy: 0.7143\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5712 - binary_accuracy: 0.7143\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 8ms/step - loss: 0.5518 - binary_accuracy: 0.7143\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5411 - binary_accuracy: 0.7257\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5235 - binary_accuracy: 0.7714\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5108 - binary_accuracy: 0.7600\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5458 - binary_accuracy: 0.7600\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5228 - binary_accuracy: 0.7429\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5024 - binary_accuracy: 0.7829\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4878 - binary_accuracy: 0.7600\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4761 - binary_accuracy: 0.7829\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4823 - binary_accuracy: 0.7600\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4606 - binary_accuracy: 0.7943\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4371 - binary_accuracy: 0.8171\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4195 - binary_accuracy: 0.8114\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4334 - binary_accuracy: 0.8286\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4159 - binary_accuracy: 0.8457\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4307 - binary_accuracy: 0.8057\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3930 - binary_accuracy: 0.8743\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3878 - binary_accuracy: 0.8400\n",
            "For the fold number  1 :\n",
            "loss =  0.4141755998134613 \n",
            "accuracy =  80.0000011920929 %\n",
            "Training the fold number  2 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 2ms/step - loss: 0.9351 - binary_accuracy: 0.5143\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7053 - binary_accuracy: 0.6343\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7251 - binary_accuracy: 0.6229\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6759 - binary_accuracy: 0.6686\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6375 - binary_accuracy: 0.6857\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5801 - binary_accuracy: 0.7314\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5748 - binary_accuracy: 0.7143\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5513 - binary_accuracy: 0.7086\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5653 - binary_accuracy: 0.7200\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5442 - binary_accuracy: 0.7543\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5262 - binary_accuracy: 0.7429\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5170 - binary_accuracy: 0.7771\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4550 - binary_accuracy: 0.8114\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4299 - binary_accuracy: 0.8343\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4591 - binary_accuracy: 0.8000\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3930 - binary_accuracy: 0.8514\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3927 - binary_accuracy: 0.8400\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3983 - binary_accuracy: 0.8743\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4049 - binary_accuracy: 0.8457\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3739 - binary_accuracy: 0.8629\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3838 - binary_accuracy: 0.8343\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3474 - binary_accuracy: 0.8800\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3829 - binary_accuracy: 0.8629\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3868 - binary_accuracy: 0.8743\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3426 - binary_accuracy: 0.8743\n",
            "For the fold number  2 :\n",
            "loss =  0.6845226287841797 \n",
            "accuracy =  68.00000071525574 %\n",
            "Training the fold number  3 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 2ms/step - loss: 0.7162 - binary_accuracy: 0.6171\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6252 - binary_accuracy: 0.7143\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5983 - binary_accuracy: 0.7314\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6143 - binary_accuracy: 0.7143\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5668 - binary_accuracy: 0.7143\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5485 - binary_accuracy: 0.7200\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5572 - binary_accuracy: 0.7200\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5229 - binary_accuracy: 0.7314\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5654 - binary_accuracy: 0.7257\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5033 - binary_accuracy: 0.7829\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5157 - binary_accuracy: 0.7657\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4733 - binary_accuracy: 0.7829\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4731 - binary_accuracy: 0.7943\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5015 - binary_accuracy: 0.7429\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4305 - binary_accuracy: 0.8171\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4106 - binary_accuracy: 0.8171\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4232 - binary_accuracy: 0.8286\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4143 - binary_accuracy: 0.8114\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3889 - binary_accuracy: 0.8400\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3704 - binary_accuracy: 0.8571\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3817 - binary_accuracy: 0.8743\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3781 - binary_accuracy: 0.8629\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3844 - binary_accuracy: 0.8857\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3323 - binary_accuracy: 0.8686\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3275 - binary_accuracy: 0.8914\n",
            "For the fold number  3 :\n",
            "loss =  0.922538161277771 \n",
            "accuracy =  60.00000238418579 %\n",
            "Training the fold number  4 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 3ms/step - loss: 1.2833 - binary_accuracy: 0.3429\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.0126 - binary_accuracy: 0.4800\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8837 - binary_accuracy: 0.5543\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8079 - binary_accuracy: 0.5486\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7412 - binary_accuracy: 0.5886\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7483 - binary_accuracy: 0.5943\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6560 - binary_accuracy: 0.6457\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6799 - binary_accuracy: 0.6686\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6196 - binary_accuracy: 0.6914\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5718 - binary_accuracy: 0.7029\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6096 - binary_accuracy: 0.6743\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5767 - binary_accuracy: 0.7371\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4989 - binary_accuracy: 0.7657\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5034 - binary_accuracy: 0.7886\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4991 - binary_accuracy: 0.7486\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4782 - binary_accuracy: 0.8057\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4773 - binary_accuracy: 0.7943\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4668 - binary_accuracy: 0.8057\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4688 - binary_accuracy: 0.8171\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4213 - binary_accuracy: 0.8343\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4044 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4178 - binary_accuracy: 0.8286\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4203 - binary_accuracy: 0.8171\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4178 - binary_accuracy: 0.8286\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3769 - binary_accuracy: 0.8743\n",
            "For the fold number  4 :\n",
            "loss =  0.7048057317733765 \n",
            "accuracy =  63.999998569488525 %\n",
            "Training the fold number  5 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 3ms/step - loss: 0.6692 - binary_accuracy: 0.6343\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6583 - binary_accuracy: 0.6514\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6353 - binary_accuracy: 0.6457\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5807 - binary_accuracy: 0.6914\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5566 - binary_accuracy: 0.7314\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5547 - binary_accuracy: 0.7657\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5649 - binary_accuracy: 0.7257\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5076 - binary_accuracy: 0.7543\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5188 - binary_accuracy: 0.7886\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5098 - binary_accuracy: 0.7943\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4854 - binary_accuracy: 0.8000\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4863 - binary_accuracy: 0.7886\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4559 - binary_accuracy: 0.8229\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4444 - binary_accuracy: 0.8343\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4326 - binary_accuracy: 0.8343\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4306 - binary_accuracy: 0.8171\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4208 - binary_accuracy: 0.8286\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4260 - binary_accuracy: 0.8286\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4135 - binary_accuracy: 0.8343\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3896 - binary_accuracy: 0.8914\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3924 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3818 - binary_accuracy: 0.8514\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3799 - binary_accuracy: 0.8571\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3735 - binary_accuracy: 0.8743\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3674 - binary_accuracy: 0.8800\n",
            "For the fold number  5 :\n",
            "loss =  0.5866672396659851 \n",
            "accuracy =  63.999998569488525 %\n",
            "Training the fold number  6 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 2ms/step - loss: 0.7661 - binary_accuracy: 0.5714\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6815 - binary_accuracy: 0.6286\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6891 - binary_accuracy: 0.6286\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6279 - binary_accuracy: 0.6571\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6339 - binary_accuracy: 0.6800\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6153 - binary_accuracy: 0.7086\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5772 - binary_accuracy: 0.6857\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5704 - binary_accuracy: 0.7200\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5754 - binary_accuracy: 0.7200\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5429 - binary_accuracy: 0.7486\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5366 - binary_accuracy: 0.7486\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4940 - binary_accuracy: 0.7657\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4855 - binary_accuracy: 0.8000\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4620 - binary_accuracy: 0.7714\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4771 - binary_accuracy: 0.7829\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4512 - binary_accuracy: 0.8171\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4600 - binary_accuracy: 0.8229\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4318 - binary_accuracy: 0.8171\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4618 - binary_accuracy: 0.8057\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4341 - binary_accuracy: 0.7943\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4000 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3875 - binary_accuracy: 0.8743\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3967 - binary_accuracy: 0.8286\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3834 - binary_accuracy: 0.8571\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3755 - binary_accuracy: 0.8457\n",
            "For the fold number  6 :\n",
            "loss =  0.7112622261047363 \n",
            "accuracy =  68.00000071525574 %\n",
            "Training the fold number  7 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 3ms/step - loss: 0.9331 - binary_accuracy: 0.4571\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7925 - binary_accuracy: 0.5257\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7635 - binary_accuracy: 0.5657\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7172 - binary_accuracy: 0.5943\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6839 - binary_accuracy: 0.6514\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6706 - binary_accuracy: 0.6229\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6164 - binary_accuracy: 0.6800\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6009 - binary_accuracy: 0.7086\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5510 - binary_accuracy: 0.6857\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5604 - binary_accuracy: 0.7314\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5512 - binary_accuracy: 0.7257\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5230 - binary_accuracy: 0.7771\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5007 - binary_accuracy: 0.7886\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4951 - binary_accuracy: 0.7600\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4853 - binary_accuracy: 0.7886\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4836 - binary_accuracy: 0.8057\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4600 - binary_accuracy: 0.8000\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4552 - binary_accuracy: 0.7943\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4585 - binary_accuracy: 0.7943\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4420 - binary_accuracy: 0.8286\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4350 - binary_accuracy: 0.8229\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4246 - binary_accuracy: 0.8286\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4181 - binary_accuracy: 0.8629\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4141 - binary_accuracy: 0.8229\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3693 - binary_accuracy: 0.8686\n",
            "For the fold number  7 :\n",
            "loss =  0.6897688508033752 \n",
            "accuracy =  60.00000238418579 %\n",
            "Training the fold number  8 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 2ms/step - loss: 0.9674 - binary_accuracy: 0.4857\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8691 - binary_accuracy: 0.5371\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7632 - binary_accuracy: 0.6114\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6997 - binary_accuracy: 0.5657\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7252 - binary_accuracy: 0.6114\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6450 - binary_accuracy: 0.6571\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6657 - binary_accuracy: 0.6457\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6033 - binary_accuracy: 0.6971\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6026 - binary_accuracy: 0.6857\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6015 - binary_accuracy: 0.7029\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5633 - binary_accuracy: 0.7314\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5393 - binary_accuracy: 0.7429\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5330 - binary_accuracy: 0.7371\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5526 - binary_accuracy: 0.7486\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4723 - binary_accuracy: 0.7771\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5196 - binary_accuracy: 0.7429\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4609 - binary_accuracy: 0.8229\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4971 - binary_accuracy: 0.7600\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4563 - binary_accuracy: 0.8286\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4459 - binary_accuracy: 0.8114\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4521 - binary_accuracy: 0.7886\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4154 - binary_accuracy: 0.8343\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4439 - binary_accuracy: 0.8057\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3983 - binary_accuracy: 0.8229\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3852 - binary_accuracy: 0.8400\n",
            "For the fold number  8 :\n",
            "loss =  0.5230752825737 \n",
            "accuracy =  80.0000011920929 %\n",
            "\n",
            "\n",
            "\n",
            "Doing the run number  2\n",
            "Training the fold number  1 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 4ms/step - loss: 0.7005 - binary_accuracy: 0.6857\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6636 - binary_accuracy: 0.6971\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6576 - binary_accuracy: 0.7143\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6397 - binary_accuracy: 0.6857\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6004 - binary_accuracy: 0.7200\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5784 - binary_accuracy: 0.7257\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5463 - binary_accuracy: 0.7371\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5041 - binary_accuracy: 0.7200\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5088 - binary_accuracy: 0.7600\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5143 - binary_accuracy: 0.7600\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4894 - binary_accuracy: 0.7657\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5238 - binary_accuracy: 0.7771\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4776 - binary_accuracy: 0.7771\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4487 - binary_accuracy: 0.8000\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4332 - binary_accuracy: 0.8229\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4561 - binary_accuracy: 0.8286\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4242 - binary_accuracy: 0.8343\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4560 - binary_accuracy: 0.7943\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4442 - binary_accuracy: 0.8114\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4114 - binary_accuracy: 0.8514\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3957 - binary_accuracy: 0.8571\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4321 - binary_accuracy: 0.8229\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4098 - binary_accuracy: 0.8514\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4094 - binary_accuracy: 0.8286\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3920 - binary_accuracy: 0.8514\n",
            "For the fold number  1 :\n",
            "loss =  0.566963791847229 \n",
            "accuracy =  72.00000286102295 %\n",
            "Training the fold number  2 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 2ms/step - loss: 0.7349 - binary_accuracy: 0.5714\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6992 - binary_accuracy: 0.6286\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6519 - binary_accuracy: 0.7029\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6053 - binary_accuracy: 0.7029\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6143 - binary_accuracy: 0.7257\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5840 - binary_accuracy: 0.7600\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5764 - binary_accuracy: 0.7257\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5821 - binary_accuracy: 0.6914\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4959 - binary_accuracy: 0.7829\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5305 - binary_accuracy: 0.7543\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5139 - binary_accuracy: 0.8057\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5283 - binary_accuracy: 0.7657\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4847 - binary_accuracy: 0.7886\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4632 - binary_accuracy: 0.8000\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4234 - binary_accuracy: 0.8457\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4516 - binary_accuracy: 0.8229\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4508 - binary_accuracy: 0.8286\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4534 - binary_accuracy: 0.8000\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4415 - binary_accuracy: 0.8114\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4286 - binary_accuracy: 0.8343\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4335 - binary_accuracy: 0.8343\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4071 - binary_accuracy: 0.8343\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4118 - binary_accuracy: 0.8514\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3829 - binary_accuracy: 0.8914\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3794 - binary_accuracy: 0.8571\n",
            "For the fold number  2 :\n",
            "loss =  0.579221785068512 \n",
            "accuracy =  63.999998569488525 %\n",
            "Training the fold number  3 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8292 - binary_accuracy: 0.5486\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7679 - binary_accuracy: 0.5429\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7631 - binary_accuracy: 0.5771\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6795 - binary_accuracy: 0.6400\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6354 - binary_accuracy: 0.6971\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6321 - binary_accuracy: 0.6743\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5964 - binary_accuracy: 0.7086\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6133 - binary_accuracy: 0.6971\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5579 - binary_accuracy: 0.7314\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5498 - binary_accuracy: 0.7257\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5781 - binary_accuracy: 0.7257\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5319 - binary_accuracy: 0.7429\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5104 - binary_accuracy: 0.7657\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5295 - binary_accuracy: 0.7829\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4430 - binary_accuracy: 0.8114\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4324 - binary_accuracy: 0.8286\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4577 - binary_accuracy: 0.8286\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4570 - binary_accuracy: 0.8114\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4548 - binary_accuracy: 0.8171\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4599 - binary_accuracy: 0.8000\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4064 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4277 - binary_accuracy: 0.8571\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4150 - binary_accuracy: 0.8686\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4173 - binary_accuracy: 0.8400\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4079 - binary_accuracy: 0.8400\n",
            "For the fold number  3 :\n",
            "loss =  0.7343869209289551 \n",
            "accuracy =  68.00000071525574 %\n",
            "Training the fold number  4 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6956 - binary_accuracy: 0.6743\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6562 - binary_accuracy: 0.7257\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6097 - binary_accuracy: 0.7143\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5689 - binary_accuracy: 0.7371\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5806 - binary_accuracy: 0.7371\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5637 - binary_accuracy: 0.7200\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5323 - binary_accuracy: 0.7657\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5166 - binary_accuracy: 0.7543\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5062 - binary_accuracy: 0.7543\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4959 - binary_accuracy: 0.7600\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4996 - binary_accuracy: 0.7771\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4631 - binary_accuracy: 0.7829\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4819 - binary_accuracy: 0.7943\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4498 - binary_accuracy: 0.8057\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4620 - binary_accuracy: 0.8171\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4280 - binary_accuracy: 0.8171\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4442 - binary_accuracy: 0.8114\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4257 - binary_accuracy: 0.8114\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4314 - binary_accuracy: 0.8343\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4095 - binary_accuracy: 0.8457\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4043 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4051 - binary_accuracy: 0.8457\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3890 - binary_accuracy: 0.8571\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3788 - binary_accuracy: 0.8800\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3827 - binary_accuracy: 0.8571\n",
            "For the fold number  4 :\n",
            "loss =  0.682504415512085 \n",
            "accuracy =  72.00000286102295 %\n",
            "Training the fold number  5 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 1.1801 - binary_accuracy: 0.3943\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9313 - binary_accuracy: 0.5086\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8076 - binary_accuracy: 0.5486\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7404 - binary_accuracy: 0.6000\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7211 - binary_accuracy: 0.5829\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6762 - binary_accuracy: 0.6343\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6520 - binary_accuracy: 0.6229\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6076 - binary_accuracy: 0.6857\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5738 - binary_accuracy: 0.6971\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5723 - binary_accuracy: 0.7314\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5360 - binary_accuracy: 0.7657\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5377 - binary_accuracy: 0.7543\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5160 - binary_accuracy: 0.7314\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5015 - binary_accuracy: 0.7886\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4941 - binary_accuracy: 0.8114\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5003 - binary_accuracy: 0.7886\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5018 - binary_accuracy: 0.8171\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4498 - binary_accuracy: 0.8286\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4906 - binary_accuracy: 0.7943\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4411 - binary_accuracy: 0.8400\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4377 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4179 - binary_accuracy: 0.8400\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4220 - binary_accuracy: 0.8400\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4008 - binary_accuracy: 0.8686\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3982 - binary_accuracy: 0.8514\n",
            "For the fold number  5 :\n",
            "loss =  0.8102973699569702 \n",
            "accuracy =  60.00000238418579 %\n",
            "Training the fold number  6 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7971 - binary_accuracy: 0.5314\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7236 - binary_accuracy: 0.5771\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6736 - binary_accuracy: 0.6000\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6033 - binary_accuracy: 0.6686\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6090 - binary_accuracy: 0.6800\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5534 - binary_accuracy: 0.7257\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5511 - binary_accuracy: 0.6914\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5555 - binary_accuracy: 0.6857\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5372 - binary_accuracy: 0.7257\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4872 - binary_accuracy: 0.7943\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4935 - binary_accuracy: 0.7829\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4628 - binary_accuracy: 0.8229\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4756 - binary_accuracy: 0.8000\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4396 - binary_accuracy: 0.8286\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4712 - binary_accuracy: 0.8000\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4529 - binary_accuracy: 0.8057\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4064 - binary_accuracy: 0.8286\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4304 - binary_accuracy: 0.8286\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4263 - binary_accuracy: 0.8400\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4176 - binary_accuracy: 0.8457\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4036 - binary_accuracy: 0.8229\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3931 - binary_accuracy: 0.8571\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3776 - binary_accuracy: 0.8571\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4019 - binary_accuracy: 0.8343\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3596 - binary_accuracy: 0.8800\n",
            "For the fold number  6 :\n",
            "loss =  0.727454423904419 \n",
            "accuracy =  60.00000238418579 %\n",
            "Training the fold number  7 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9110 - binary_accuracy: 0.4629\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8231 - binary_accuracy: 0.6229\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6942 - binary_accuracy: 0.6343\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6762 - binary_accuracy: 0.6457\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6577 - binary_accuracy: 0.6743\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6012 - binary_accuracy: 0.6857\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5874 - binary_accuracy: 0.7486\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5835 - binary_accuracy: 0.7314\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5204 - binary_accuracy: 0.7657\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5501 - binary_accuracy: 0.7143\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5443 - binary_accuracy: 0.7429\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5181 - binary_accuracy: 0.7829\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5155 - binary_accuracy: 0.7886\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4993 - binary_accuracy: 0.7886\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4363 - binary_accuracy: 0.8343\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4814 - binary_accuracy: 0.8229\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4717 - binary_accuracy: 0.8171\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4350 - binary_accuracy: 0.8343\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4248 - binary_accuracy: 0.8400\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4308 - binary_accuracy: 0.8457\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4212 - binary_accuracy: 0.8286\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3929 - binary_accuracy: 0.8400\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4326 - binary_accuracy: 0.8343\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3703 - binary_accuracy: 0.8571\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3792 - binary_accuracy: 0.8571\n",
            "For the fold number  7 :\n",
            "loss =  0.5110129714012146 \n",
            "accuracy =  80.0000011920929 %\n",
            "Training the fold number  8 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7188 - binary_accuracy: 0.7086\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7262 - binary_accuracy: 0.6571\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6850 - binary_accuracy: 0.6800\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5759 - binary_accuracy: 0.6971\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5798 - binary_accuracy: 0.7429\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6166 - binary_accuracy: 0.7257\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5515 - binary_accuracy: 0.7829\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5671 - binary_accuracy: 0.7714\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5474 - binary_accuracy: 0.7486\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5179 - binary_accuracy: 0.7543\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5156 - binary_accuracy: 0.7943\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4927 - binary_accuracy: 0.7829\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4583 - binary_accuracy: 0.7771\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4730 - binary_accuracy: 0.8114\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4204 - binary_accuracy: 0.8343\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4225 - binary_accuracy: 0.8457\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4154 - binary_accuracy: 0.8571\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4053 - binary_accuracy: 0.8229\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3782 - binary_accuracy: 0.8629\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4273 - binary_accuracy: 0.8286\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3556 - binary_accuracy: 0.8571\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3739 - binary_accuracy: 0.8743\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3883 - binary_accuracy: 0.8457\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3574 - binary_accuracy: 0.8743\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3395 - binary_accuracy: 0.8800\n",
            "For the fold number  8 :\n",
            "loss =  0.5120906829833984 \n",
            "accuracy =  75.99999904632568 %\n",
            "\n",
            "\n",
            "\n",
            "Doing the run number  3\n",
            "Training the fold number  1 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 2ms/step - loss: 0.7127 - binary_accuracy: 0.7086\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7454 - binary_accuracy: 0.6514\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6914 - binary_accuracy: 0.7086\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7352 - binary_accuracy: 0.6514\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5876 - binary_accuracy: 0.7486\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5779 - binary_accuracy: 0.7600\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6001 - binary_accuracy: 0.7200\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5338 - binary_accuracy: 0.7486\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5198 - binary_accuracy: 0.7771\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5463 - binary_accuracy: 0.7886\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5188 - binary_accuracy: 0.7714\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4695 - binary_accuracy: 0.7829\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4919 - binary_accuracy: 0.8114\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5268 - binary_accuracy: 0.8057\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4393 - binary_accuracy: 0.8229\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4868 - binary_accuracy: 0.8171\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4607 - binary_accuracy: 0.8000\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4258 - binary_accuracy: 0.8286\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4400 - binary_accuracy: 0.7886\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4107 - binary_accuracy: 0.8286\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4139 - binary_accuracy: 0.8457\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4351 - binary_accuracy: 0.8343\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3972 - binary_accuracy: 0.8686\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4057 - binary_accuracy: 0.8571\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3958 - binary_accuracy: 0.8571\n",
            "For the fold number  1 :\n",
            "loss =  0.8134793043136597 \n",
            "accuracy =  63.999998569488525 %\n",
            "Training the fold number  2 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.9753 - binary_accuracy: 0.4571\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.8587 - binary_accuracy: 0.4857\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7978 - binary_accuracy: 0.5600\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7769 - binary_accuracy: 0.5200\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7020 - binary_accuracy: 0.6000\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6494 - binary_accuracy: 0.6571\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6257 - binary_accuracy: 0.6514\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5983 - binary_accuracy: 0.6857\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5773 - binary_accuracy: 0.7314\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5497 - binary_accuracy: 0.7086\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5430 - binary_accuracy: 0.7943\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5080 - binary_accuracy: 0.7657\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4931 - binary_accuracy: 0.7714\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4868 - binary_accuracy: 0.7600\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5026 - binary_accuracy: 0.7886\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4785 - binary_accuracy: 0.7714\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4585 - binary_accuracy: 0.8000\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4846 - binary_accuracy: 0.7657\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4956 - binary_accuracy: 0.7714\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4287 - binary_accuracy: 0.8171\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4092 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4117 - binary_accuracy: 0.8400\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4089 - binary_accuracy: 0.8171\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3874 - binary_accuracy: 0.8743\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4089 - binary_accuracy: 0.8229\n",
            "For the fold number  2 :\n",
            "loss =  0.6936473846435547 \n",
            "accuracy =  72.00000286102295 %\n",
            "Training the fold number  3 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8694 - binary_accuracy: 0.5143\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7347 - binary_accuracy: 0.6114\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6886 - binary_accuracy: 0.6629\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6571 - binary_accuracy: 0.6629\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6462 - binary_accuracy: 0.6743\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6280 - binary_accuracy: 0.6857\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5759 - binary_accuracy: 0.7200\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5627 - binary_accuracy: 0.7371\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5822 - binary_accuracy: 0.7486\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5100 - binary_accuracy: 0.7371\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5381 - binary_accuracy: 0.7600\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5010 - binary_accuracy: 0.7771\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5381 - binary_accuracy: 0.7543\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4804 - binary_accuracy: 0.8114\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4676 - binary_accuracy: 0.8000\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4706 - binary_accuracy: 0.8057\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4540 - binary_accuracy: 0.8057\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4425 - binary_accuracy: 0.8171\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4304 - binary_accuracy: 0.8514\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4032 - binary_accuracy: 0.8229\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4443 - binary_accuracy: 0.8229\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3918 - binary_accuracy: 0.8743\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3760 - binary_accuracy: 0.8743\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3747 - binary_accuracy: 0.8686\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3751 - binary_accuracy: 0.8629\n",
            "For the fold number  3 :\n",
            "loss =  0.48634374141693115 \n",
            "accuracy =  80.0000011920929 %\n",
            "Training the fold number  4 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 3ms/step - loss: 0.6429 - binary_accuracy: 0.6971\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6345 - binary_accuracy: 0.7543\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5747 - binary_accuracy: 0.7714\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5833 - binary_accuracy: 0.7429\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5740 - binary_accuracy: 0.7600\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5331 - binary_accuracy: 0.7543\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.5061 - binary_accuracy: 0.7543\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4834 - binary_accuracy: 0.7829\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4977 - binary_accuracy: 0.7829\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5294 - binary_accuracy: 0.7771\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4947 - binary_accuracy: 0.7943\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.4875 - binary_accuracy: 0.8000\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4285 - binary_accuracy: 0.8286\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4415 - binary_accuracy: 0.8343\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4171 - binary_accuracy: 0.8457\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4403 - binary_accuracy: 0.7943\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4247 - binary_accuracy: 0.8514\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3682 - binary_accuracy: 0.8686\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3987 - binary_accuracy: 0.8400\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3860 - binary_accuracy: 0.8686\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3796 - binary_accuracy: 0.8571\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3863 - binary_accuracy: 0.8743\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3578 - binary_accuracy: 0.8800\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3461 - binary_accuracy: 0.8571\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3669 - binary_accuracy: 0.8857\n",
            "For the fold number  4 :\n",
            "loss =  0.8522486686706543 \n",
            "accuracy =  68.00000071525574 %\n",
            "Training the fold number  5 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7387 - binary_accuracy: 0.6629\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7029 - binary_accuracy: 0.6629\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6671 - binary_accuracy: 0.6971\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6186 - binary_accuracy: 0.7086\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5814 - binary_accuracy: 0.7257\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5570 - binary_accuracy: 0.7371\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5707 - binary_accuracy: 0.7257\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5678 - binary_accuracy: 0.7486\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5217 - binary_accuracy: 0.7600\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5263 - binary_accuracy: 0.7543\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5413 - binary_accuracy: 0.7486\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5168 - binary_accuracy: 0.7714\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4581 - binary_accuracy: 0.8114\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4640 - binary_accuracy: 0.8343\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4366 - binary_accuracy: 0.8000\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4571 - binary_accuracy: 0.8400\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4354 - binary_accuracy: 0.8457\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4196 - binary_accuracy: 0.8229\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4423 - binary_accuracy: 0.8400\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4107 - binary_accuracy: 0.8571\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4130 - binary_accuracy: 0.8629\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3873 - binary_accuracy: 0.8571\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4151 - binary_accuracy: 0.8343\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3951 - binary_accuracy: 0.8629\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3560 - binary_accuracy: 0.8743\n",
            "For the fold number  5 :\n",
            "loss =  0.8376980423927307 \n",
            "accuracy =  51.99999809265137 %\n",
            "Training the fold number  6 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7769 - binary_accuracy: 0.5829\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7074 - binary_accuracy: 0.6000\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6727 - binary_accuracy: 0.6400\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6473 - binary_accuracy: 0.6400\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6351 - binary_accuracy: 0.6457\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6308 - binary_accuracy: 0.6457\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6021 - binary_accuracy: 0.7143\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5695 - binary_accuracy: 0.7029\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5808 - binary_accuracy: 0.7200\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5101 - binary_accuracy: 0.7657\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5370 - binary_accuracy: 0.7543\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4924 - binary_accuracy: 0.7600\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4904 - binary_accuracy: 0.7829\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4889 - binary_accuracy: 0.7771\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.5105 - binary_accuracy: 0.8000\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4654 - binary_accuracy: 0.7943\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4706 - binary_accuracy: 0.8286\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4423 - binary_accuracy: 0.7886\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4298 - binary_accuracy: 0.8571\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4400 - binary_accuracy: 0.8229\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4263 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4252 - binary_accuracy: 0.8343\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4360 - binary_accuracy: 0.8229\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.3820 - binary_accuracy: 0.8629\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3784 - binary_accuracy: 0.8571\n",
            "For the fold number  6 :\n",
            "loss =  0.5722492337226868 \n",
            "accuracy =  63.999998569488525 %\n",
            "Training the fold number  7 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 1s 2ms/step - loss: 0.9195 - binary_accuracy: 0.4514\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.8242 - binary_accuracy: 0.5200\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7638 - binary_accuracy: 0.5371\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.7252 - binary_accuracy: 0.6114\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.7161 - binary_accuracy: 0.5943\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6330 - binary_accuracy: 0.6686\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6398 - binary_accuracy: 0.6400\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6548 - binary_accuracy: 0.6686\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6038 - binary_accuracy: 0.6629\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5945 - binary_accuracy: 0.7200\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5361 - binary_accuracy: 0.7600\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5846 - binary_accuracy: 0.7200\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5348 - binary_accuracy: 0.7829\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5360 - binary_accuracy: 0.7200\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5038 - binary_accuracy: 0.7771\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4910 - binary_accuracy: 0.7943\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5198 - binary_accuracy: 0.7714\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4495 - binary_accuracy: 0.8171\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4810 - binary_accuracy: 0.8114\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4759 - binary_accuracy: 0.8000\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.4335 - binary_accuracy: 0.8400\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4414 - binary_accuracy: 0.8114\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4545 - binary_accuracy: 0.8114\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4205 - binary_accuracy: 0.8400\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4065 - binary_accuracy: 0.8629\n",
            "For the fold number  7 :\n",
            "loss =  0.5838721394538879 \n",
            "accuracy =  72.00000286102295 %\n",
            "Training the fold number  8 \n",
            "\n",
            "Epoch 1/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7496 - binary_accuracy: 0.5600\n",
            "Epoch 2/25\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6820 - binary_accuracy: 0.6343\n",
            "Epoch 3/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6538 - binary_accuracy: 0.6971\n",
            "Epoch 4/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6317 - binary_accuracy: 0.6857\n",
            "Epoch 5/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5599 - binary_accuracy: 0.7029\n",
            "Epoch 6/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.6117 - binary_accuracy: 0.7029\n",
            "Epoch 7/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5404 - binary_accuracy: 0.7600\n",
            "Epoch 8/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5322 - binary_accuracy: 0.7314\n",
            "Epoch 9/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.5391 - binary_accuracy: 0.7771\n",
            "Epoch 10/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5221 - binary_accuracy: 0.7657\n",
            "Epoch 11/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4746 - binary_accuracy: 0.8114\n",
            "Epoch 12/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4852 - binary_accuracy: 0.7886\n",
            "Epoch 13/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4494 - binary_accuracy: 0.8171\n",
            "Epoch 14/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4899 - binary_accuracy: 0.8057\n",
            "Epoch 15/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4406 - binary_accuracy: 0.8400\n",
            "Epoch 16/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4624 - binary_accuracy: 0.8057\n",
            "Epoch 17/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4582 - binary_accuracy: 0.8229\n",
            "Epoch 18/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4468 - binary_accuracy: 0.8229\n",
            "Epoch 19/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4232 - binary_accuracy: 0.8400\n",
            "Epoch 20/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3980 - binary_accuracy: 0.8571\n",
            "Epoch 21/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3806 - binary_accuracy: 0.8629\n",
            "Epoch 22/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4033 - binary_accuracy: 0.8629\n",
            "Epoch 23/25\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.4012 - binary_accuracy: 0.8400\n",
            "Epoch 24/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3687 - binary_accuracy: 0.8571\n",
            "Epoch 25/25\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3905 - binary_accuracy: 0.8457\n",
            "For the fold number  8 :\n",
            "loss =  0.6614562273025513 \n",
            "accuracy =  63.999998569488525 %\n",
            "\n",
            "\n",
            "\n",
            "We obtain the following results:\n",
            "mean accuracy:  68.00000071525574 % \n",
            "mean loss: 0.6609059510131677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**CNN**"
      ],
      "metadata": {
        "id": "L9p427_DRxcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_2():\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(layers.Input(shape=(14,35)))\n",
        "\n",
        "  model.add(layers.Conv2D())"
      ],
      "metadata": {
        "id": "rFAEJl_AR0b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# **Random Forest Classifier**"
      ],
      "metadata": {
        "id": "lDHoDrLRgxXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_rfc = dataset.reshape(( dataset.shape[0], dataset.shape[1]*dataset.shape[2] ))\n",
        "print(dataset.shape)\n",
        "print(data_rfc.shape)\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features='sqrt', oob_score=True, random_state=8)\n",
        "rfc.fit(data_rfc, labels)\n",
        "\n",
        "print(rfc.oob_score_)\n",
        "print(rfc.feature_importances_)\n",
        "plt.plot(rfc.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6dQA6rQZg2mZ",
        "outputId": "0f727c81-b448-48bc-b716-feffbe3f86fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 15, 35)\n",
            "(200, 525)\n",
            "0.68\n",
            "[1.04704749e-03 2.58518819e-03 9.03176307e-04 8.70006118e-04\n",
            " 7.03001350e-04 6.28901473e-04 9.61448401e-04 1.99887482e-03\n",
            " 1.08921202e-03 8.00918442e-04 9.96865324e-04 1.51063506e-03\n",
            " 3.39739095e-04 2.59466584e-03 3.47204574e-03 1.49312767e-03\n",
            " 1.31004004e-03 3.88875197e-03 1.30295614e-03 5.71978337e-04\n",
            " 1.91133198e-03 7.76733225e-04 1.38774205e-03 2.87925595e-03\n",
            " 4.54528492e-04 1.04233399e-03 1.21895665e-04 3.25053681e-03\n",
            " 3.51561591e-04 1.36092943e-03 1.17563126e-03 1.21624907e-03\n",
            " 1.94431824e-03 5.10532418e-03 2.54129531e-03 7.44081141e-04\n",
            " 1.03806475e-03 1.23885207e-03 1.74516654e-03 5.07392403e-04\n",
            " 9.39833843e-04 7.62885748e-04 2.95045014e-03 6.30407860e-04\n",
            " 1.79255102e-03 1.55880507e-03 1.03109878e-03 5.90842203e-04\n",
            " 1.13939781e-03 1.19798702e-03 1.70469938e-03 1.21006116e-03\n",
            " 3.10534152e-03 1.54909297e-03 3.96408529e-04 1.07484457e-03\n",
            " 1.84965157e-03 2.17161590e-03 1.60954887e-03 6.86910285e-04\n",
            " 2.49017254e-03 1.58995952e-04 1.36569301e-03 4.63360600e-04\n",
            " 1.02449180e-03 1.27536156e-03 2.22495248e-03 3.27128856e-03\n",
            " 2.58707549e-03 2.96671920e-03 7.62042533e-04 1.08858187e-03\n",
            " 9.14959365e-04 6.53409238e-04 8.68154221e-04 1.72413560e-03\n",
            " 1.63219971e-03 1.28276711e-03 7.02282918e-04 1.72040179e-03\n",
            " 1.58601998e-03 1.19394968e-03 1.26617216e-03 2.38585528e-03\n",
            " 2.08505954e-03 2.19139529e-03 1.31583277e-03 5.62084169e-03\n",
            " 9.80635671e-04 3.12629690e-04 2.13838487e-03 1.39665031e-03\n",
            " 1.57378451e-03 1.61214887e-03 3.49161605e-04 2.24345603e-03\n",
            " 3.21388403e-04 1.75526166e-03 6.13164643e-04 1.61303733e-03\n",
            " 1.55748518e-03 1.92092416e-03 1.27808308e-03 1.12880927e-03\n",
            " 2.01730567e-03 1.19099556e-03 1.48091780e-03 6.11566362e-04\n",
            " 9.55523480e-04 3.48225885e-04 8.69767803e-04 9.44057226e-04\n",
            " 1.83962306e-03 1.24585412e-03 7.93834499e-04 1.24991348e-03\n",
            " 1.31050027e-03 1.08025520e-03 3.52817894e-03 3.73661191e-03\n",
            " 2.33359755e-03 1.56475937e-03 1.67491892e-03 1.39464148e-03\n",
            " 1.24303851e-03 1.73996545e-03 1.75519607e-03 1.52139139e-03\n",
            " 1.80383548e-03 3.78990985e-04 1.51797187e-03 8.74547437e-05\n",
            " 1.23306648e-03 2.53538052e-04 1.27330878e-03 1.69073851e-03\n",
            " 1.07100546e-03 1.39423812e-03 1.71964457e-03 1.91929680e-03\n",
            " 1.93394252e-03 2.33036914e-03 7.96792809e-04 7.12629295e-04\n",
            " 9.29146500e-04 8.17226069e-04 1.13477824e-03 7.62999720e-03\n",
            " 9.21017676e-04 9.54804018e-04 8.92039553e-04 5.77661139e-04\n",
            " 3.01119355e-03 1.13482368e-03 1.55051964e-03 1.71865079e-03\n",
            " 1.95480871e-03 2.25432265e-03 1.28752594e-03 4.29018830e-04\n",
            " 1.47644784e-03 2.39992448e-03 2.66093583e-03 2.38954572e-03\n",
            " 1.84369782e-04 2.27720145e-03 1.88172768e-04 2.43248010e-03\n",
            " 2.48726210e-03 1.11170314e-03 1.57716981e-03 5.70167841e-04\n",
            " 1.57449800e-03 2.50396657e-03 2.27648547e-03 1.74840147e-03\n",
            " 2.75751797e-03 8.40311611e-04 1.29028731e-03 5.46348111e-04\n",
            " 1.02772344e-03 1.03570556e-03 3.02402672e-03 1.01471542e-03\n",
            " 1.04927544e-03 8.70829076e-04 8.46241431e-04 6.94995812e-04\n",
            " 2.46903670e-03 2.89326563e-03 3.97838902e-03 9.14946581e-04\n",
            " 3.96074023e-03 1.46894496e-03 2.69122299e-03 1.50275083e-03\n",
            " 2.76621796e-03 2.06751138e-03 3.38502886e-03 3.82214186e-04\n",
            " 1.70940657e-03 6.35575004e-04 1.83071692e-03 1.84614831e-04\n",
            " 1.57603000e-03 1.53776442e-03 1.35691278e-03 1.85591667e-03\n",
            " 2.06531559e-03 3.18018889e-03 6.04093774e-04 9.45828387e-04\n",
            " 9.41280552e-04 1.09100204e-03 1.00132881e-04 9.13365881e-04\n",
            " 1.48510849e-03 1.50082552e-03 9.38860677e-04 7.43041880e-04\n",
            " 1.09520532e-03 1.41058947e-03 1.14081261e-03 3.57698379e-03\n",
            " 5.16848814e-03 4.24076887e-03 1.17777164e-03 3.50827319e-03\n",
            " 9.72266239e-04 3.89097746e-04 1.61506412e-03 2.29088464e-03\n",
            " 1.30613296e-03 2.02991845e-03 1.49575899e-04 1.18889471e-03\n",
            " 3.44300433e-04 2.05535379e-03 8.67768431e-04 1.40401205e-03\n",
            " 1.58732069e-03 8.74710478e-04 1.95583684e-03 1.18401569e-03\n",
            " 1.99636913e-03 8.52077985e-04 1.46686834e-03 1.06518899e-03\n",
            " 1.51662609e-03 6.35564415e-04 1.63695237e-03 1.05380991e-03\n",
            " 2.57767288e-03 1.46653110e-03 1.33381643e-03 1.11327044e-03\n",
            " 2.36091553e-03 1.43447033e-03 2.30785872e-03 3.23773524e-03\n",
            " 4.52851184e-03 2.17299907e-03 2.10903218e-03 1.16945610e-03\n",
            " 1.38225888e-03 1.59780267e-03 1.32193346e-03 1.22608037e-03\n",
            " 3.03670832e-03 4.34828779e-04 2.98574890e-03 3.53420464e-04\n",
            " 4.57021917e-03 7.49136257e-04 2.18576160e-03 1.01723751e-03\n",
            " 8.70757417e-04 1.42075116e-03 1.64563527e-03 1.19576561e-03\n",
            " 7.68609402e-04 1.44486838e-03 1.21933523e-03 1.52854783e-03\n",
            " 3.77017294e-04 1.25094100e-03 1.45083263e-03 1.39511409e-03\n",
            " 1.20033193e-03 6.52879192e-04 1.00592773e-03 8.72352713e-04\n",
            " 7.55365583e-04 3.69179905e-03 3.06063709e-03 3.10794836e-03\n",
            " 1.75026353e-03 2.85860087e-03 1.31015419e-03 2.89463292e-04\n",
            " 2.57710484e-03 2.31165468e-03 1.01992767e-03 1.24580022e-03\n",
            " 6.63827678e-04 3.86367099e-03 2.08622304e-04 1.76635356e-03\n",
            " 4.36730605e-04 1.14569236e-03 1.45062310e-03 7.73952581e-04\n",
            " 1.40672740e-03 1.57498412e-03 1.35940285e-03 1.71961713e-03\n",
            " 1.48175823e-03 1.37787180e-03 2.27612986e-03 1.24429527e-03\n",
            " 1.12397427e-03 1.59642206e-03 2.30711630e-03 1.04278924e-03\n",
            " 1.07371124e-03 1.58592888e-03 1.32822600e-03 1.10624117e-03\n",
            " 2.07812649e-03 3.18235838e-03 4.01529085e-03 1.51328334e-03\n",
            " 1.64632021e-03 2.65451249e-03 3.94006190e-04 1.03852591e-03\n",
            " 1.10070536e-03 1.26360307e-03 1.94396114e-03 7.22738134e-04\n",
            " 1.70875567e-03 2.71530516e-04 1.60677652e-03 3.80207524e-04\n",
            " 1.40208602e-03 2.04365734e-03 9.87845027e-04 2.13248873e-03\n",
            " 1.26953757e-03 1.03776656e-03 9.25066589e-04 1.13459962e-03\n",
            " 5.93705061e-04 6.74417743e-04 1.44647967e-03 1.39711284e-03\n",
            " 1.61551376e-03 1.25068077e-03 1.01307258e-03 8.55297456e-04\n",
            " 1.05813951e-03 7.29616576e-04 1.12021616e-03 1.54340173e-03\n",
            " 1.84549086e-03 1.90371558e-03 2.25927009e-03 1.25378590e-03\n",
            " 2.73764723e-03 4.28493016e-04 1.79551992e-03 2.04431424e-03\n",
            " 1.52563960e-03 1.62902638e-03 2.43545404e-04 1.28485844e-03\n",
            " 5.02812751e-04 2.94235344e-03 2.21283804e-04 1.29974477e-03\n",
            " 2.37395105e-03 7.00447797e-04 9.61698433e-04 1.52652453e-03\n",
            " 1.94133167e-03 8.77903554e-04 1.99213759e-03 7.83040612e-04\n",
            " 7.90545813e-04 2.48597088e-04 1.12288415e-03 1.07952082e-03\n",
            " 1.59817579e-03 1.28805426e-03 1.18878092e-03 1.13245206e-03\n",
            " 1.15472437e-03 1.15499150e-03 1.79413818e-03 1.61355117e-03\n",
            " 2.31914141e-03 2.06520702e-03 1.83102218e-03 4.36697907e-03\n",
            " 2.84267117e-04 9.24804997e-04 1.82840536e-03 1.29400742e-03\n",
            " 1.10802327e-03 3.44259033e-04 2.06825989e-03 3.05267475e-04\n",
            " 2.90383126e-03 4.32271902e-05 1.23504786e-03 2.33150429e-03\n",
            " 1.74987906e-03 1.13316110e-03 2.70727938e-03 2.39002965e-03\n",
            " 7.59181243e-04 1.40048178e-03 8.77271938e-04 5.71656919e-04\n",
            " 1.64617102e-03 7.80374572e-04 5.98609278e-04 1.11198173e-03\n",
            " 7.67475845e-04 9.63466637e-04 1.12993576e-03 5.69739014e-04\n",
            " 9.66816995e-04 2.32634975e-03 1.74085933e-03 1.29153731e-03\n",
            " 2.02460983e-03 1.95924042e-03 1.64208332e-03 1.55581272e-04\n",
            " 1.01413433e-03 1.79928882e-03 2.33676548e-03 8.81366066e-04\n",
            " 1.91591670e-04 1.70830856e-03 1.92258113e-04 1.75045416e-03\n",
            " 4.24196313e-04 1.31031897e-03 1.62416280e-03 1.56558911e-03\n",
            " 1.83537291e-03 2.15857384e-03 1.61117977e-03 4.59953679e-04\n",
            " 1.67471297e-03 1.04744371e-03 1.11757915e-03 3.94831099e-04\n",
            " 1.06094519e-03 9.46969041e-04 2.52434789e-03 1.26155749e-03\n",
            " 1.37748740e-03 2.31143526e-03 1.57048161e-03 9.65770909e-04\n",
            " 4.41940559e-03 5.01637131e-03 6.63569995e-03 9.56870875e-04\n",
            " 1.07965824e-03 1.07480624e-03 9.56543554e-04 1.27282405e-03\n",
            " 1.59656867e-03 1.02437263e-03 1.03349955e-03 1.50763899e-04\n",
            " 2.06507166e-03 3.20149367e-04 1.66643819e-03 5.08763473e-04\n",
            " 1.56066552e-03 1.70454057e-03 1.36629504e-03 2.43719129e-03\n",
            " 2.80630756e-03 1.67655685e-03 8.82716682e-03 9.63470571e-03\n",
            " 1.05741702e-02 1.24666098e-02 5.30863020e-03 5.60243177e-03\n",
            " 7.26016608e-03 1.84128971e-02 7.65012999e-03 8.03376953e-03\n",
            " 5.53950760e-03 7.04740047e-03 5.46984197e-03 7.02557233e-03\n",
            " 1.03719116e-02 1.31002812e-02 5.27891079e-03 5.29768036e-03\n",
            " 7.48686794e-03 4.39901093e-03 6.03679746e-03 5.50741078e-03\n",
            " 5.51525012e-03 5.46957031e-03 2.21004231e-03 3.51294932e-03\n",
            " 2.80240270e-03 5.85249549e-03 5.53882748e-03 6.31961796e-03\n",
            " 6.77117619e-03 6.95954717e-03 6.54311770e-03 8.93616465e-03\n",
            " 1.29478041e-02]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2b108988d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}